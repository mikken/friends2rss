#!/usr/bin/python3
"""This script loads LJ cookies and then generates RSS feed from friends page.

Author: Pavel Volkov
"""

import sys
import configparser
import urllib.request
import urllib.error
import http.cookiejar
import http.client
from os.path import abspath, realpath, dirname
from os import chdir

from lxml import etree

from friendsaux import rss_builder

def open_url():
    """Retreives friends page and returns the response."""
    if not keep_quiet:
        print('opening page', URL)
    try:
        return opener.open(URL)
    except urllib.error.URLError:
        sys.stderr.write('error while loading page\n')
        sys.exit(2)


def read_response():
    try:
        response = open_url()
        tree = etree.parse(response, parser=etree.HTMLParser())
    except http.client.IncompleteRead:
        sys.stderr.write('Could not read server response\n')
        sys.exit(27)
    return tree


def parse_page(soup, entries):
    """Goes through a page and generates a list with entries."""
    articles = soup.findall('.//article[@data-journal]')
    for article in articles:
        entry = rss_builder.Entry()
        entry.author = article.get('data-journal')
        date_tag = article.find('.//span[@class="b-lenta-item-date"]')
        entry.date = date_tag.text
        title_tag = article.find('.//h3[@class="b-lenta-item-title"]/a')
        entry.subject = title_tag.text
        protected_tag = article.find(\
          './header/div/ul[@class="b-item-type"]/li[@title="Friends-only"]')
        if (protected_tag is not None):
            entry.subject = protected_prefix + ' ' + entry.subject
        entry.link = title_tag.get('href')
        entrytext_tag = article.find('.//div[@class="b-lenta-item-content"]')
        # iframes vary between refetches, we strip them
        for iframe_tag in entrytext_tag.findall('.//iframe'):
            iframe_tag.getparent().replace(iframe_tag, etree.XML('<b>(IFRAME)</b>'))
        # strip likes
        for like in entrytext_tag.findall('.//div[@class="lj-like"]'):
            like.getparent().remove(like)
        # need to strip <div> and </div>
        content = etree.tostring(entrytext_tag, with_tail=False, \
                encoding='utf-8').decode('utf-8')
        pos = content.find('>')
        content = content[pos + 1:-6].strip()
        entry.text=content
        entries.append(entry)


def check_logged_state(soup):
    """Checks if we are logged in by analyzing HTML page.
    Returns True in case of a yes."""
    mark_tag = soup.find('logged-out"')
    if mark_tag is None:
        return True
    else:
        return False


def main():
    """Script entry point"""
    global URL, opener, protected_prefix
    # First, set working directory to current .py's location
    chdir(dirname(realpath(abspath(__file__))))
    # Load config
    try:
        config = configparser.ConfigParser()
        config.read('friends2rss.conf')
        URL = config['global']['URL']
        initialURL = URL
        depth = int(config['global']['depth'])
        protected_prefix = config['global']['protected_prefix']
    except IOError:
        sys.stderr.write('Could not read config file\n')
        sys.exit(3)
    except (KeyError, ValueError):
        sys.stderr.write("Config file isn't sane\n")
        sys.exit(4)
    # Load cookies generated by login script
    jar = http.cookiejar.MozillaCookieJar()
    try:
        jar.load('lj_cookies.txt')
    except IOError:
        sys.stderr.write('Error loading cookies\n')
        sys.exit(1)
    cooker = urllib.request.HTTPCookieProcessor(jar)
    opener = urllib.request.build_opener(cooker)
    tree=read_response()
    soup = tree.getroot()
    if not check_logged_state(soup):
        sys.stderr.write('Not logged in\n')
        sys.exit(26)
    title = soup.find('.//head/title').text
    userpic_tag = soup.find('.//div[@class="s-userpic"]')
    userpic = userpic_tag.get('style').split('(')[-1][:-1]
    alink = soup.find('.//a[@class="l-flatslide-menu-link"]').get('href')
    image = {'url' : userpic, \
            'link' : alink, \
            'title' : 'image', \
            'width' : '100', \
            'height' : '100'}
    entries = []
    parse_page(soup, entries)
    skip_step = len(entries)
    skip = 0
    depth -= 1
    while depth > 0:
        skip += skip_step
        URL = initialURL + '?skip=' + str(skip)
        soup = read_response().getroot()
        parse_page(soup, entries)
        depth -= 1
    rss_feed = rss_builder.build_rss(entries, title, initialURL, \
            'livejournal friends feed', image)
    with open('friends.xml', 'w', encoding='utf-8') as f:
        f.write(rss_feed)


if __name__ == '__main__':
    usage = 'Usage: friend2rss.py [-q]\n\n'
    keep_quiet = False
    for argument in sys.argv[1:]:
        if argument == '-q':
            keep_quiet = True
        else:
            sys.stderr.write(usage)
            sys.exit(25)
    main()
